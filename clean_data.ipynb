{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d793c0a48251aa2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import date_range_data_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:44:33.713031100Z",
     "start_time": "2023-11-10T17:44:32.287807200Z"
    }
   },
   "id": "56605910302ac169"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0c7e97707f1c9f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "Y = '2023'\n",
    "quoter = 3\n",
    "\n",
    "if quoter == 1:\n",
    "    Q = ['Q1','01-01','03-31']\n",
    "elif quoter == 2:\n",
    "    Q = ['Q2','04-01','06-30']\n",
    "elif quoter == 3:\n",
    "    Q = ['Q3','07-01','09-30']\n",
    "elif quoter == 4:\n",
    "    Q = ['Q4','10-01','12-31']\n",
    "\n",
    "from_date = f'{Y}-{Q[1]}' # ex: '2021-01-01'\n",
    "to_date = f'{Y}-{Q[2]}' # ex: '2021-03-31'\n",
    "output_filename = f'{Y}-{Q[0]}_seconds.csv' # ex: '2021-Q1_seconds.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:44:33.765514Z",
     "start_time": "2023-11-10T17:44:33.721574Z"
    }
   },
   "id": "4c881350b100140f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c1e8a1ac234188"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:45:52.610070100Z",
     "start_time": "2023-11-10T17:44:33.748520600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file 2023-07-01.csv...\n",
      "Loading file 2023-07-02.csv...\n",
      "Loading file 2023-07-03.csv...\n",
      "Loading file 2023-07-04.csv...\n",
      "Loading file 2023-07-05.csv...\n",
      "Loading file 2023-07-06.csv...\n",
      "Loading file 2023-07-07.csv...\n",
      "Loading file 2023-07-08.csv...\n",
      "Loading file 2023-07-09.csv...\n",
      "Loading file 2023-07-10.csv...\n",
      "Loading file 2023-07-11.csv...\n",
      "Loading file 2023-07-12.csv...\n",
      "Loading file 2023-07-13.csv...\n",
      "Loading file 2023-07-14.csv...\n",
      "Loading file 2023-07-15.csv...\n",
      "Loading file 2023-07-16.csv...\n",
      "Loading file 2023-07-17.csv...\n",
      "Loading file 2023-07-18.csv...\n",
      "Loading file 2023-07-19.csv...\n",
      "Loading file 2023-07-20.csv...\n",
      "Loading file 2023-07-21.csv...\n",
      "Loading file 2023-07-22.csv...\n",
      "Loading file 2023-07-23.csv...\n",
      "Loading file 2023-07-24.csv...\n",
      "Loading file 2023-07-25.csv...\n",
      "Loading file 2023-07-26.csv...\n",
      "Loading file 2023-07-27.csv...\n",
      "Loading file 2023-07-28.csv...\n",
      "Loading file 2023-07-29.csv...\n",
      "Loading file 2023-07-30.csv...\n",
      "Loading file 2023-07-31.csv...\n",
      "Loading file 2023-08-01.csv...\n",
      "Loading file 2023-08-02.csv...\n",
      "Loading file 2023-08-03.csv...\n",
      "Loading file 2023-08-04.csv...\n",
      "Loading file 2023-08-05.csv...\n",
      "Loading file 2023-08-06.csv...\n",
      "Loading file 2023-08-07.csv...\n",
      "Loading file 2023-08-08.csv...\n",
      "Loading file 2023-08-09.csv...\n",
      "Loading file 2023-08-10.csv...\n",
      "Loading file 2023-08-11.csv...\n",
      "Loading file 2023-08-12.csv...\n",
      "Loading file 2023-08-13.csv...\n",
      "Loading file 2023-08-14.csv...\n",
      "Loading file 2023-08-15.csv...\n",
      "Loading file 2023-08-16.csv...\n",
      "Loading file 2023-08-17.csv...\n",
      "Loading file 2023-08-18.csv...\n",
      "Loading file 2023-08-19.csv...\n",
      "Loading file 2023-08-20.csv...\n",
      "Loading file 2023-08-21.csv...\n",
      "Loading file 2023-08-22.csv...\n",
      "Loading file 2023-08-23.csv...\n",
      "Loading file 2023-08-24.csv...\n",
      "Loading file 2023-08-25.csv...\n",
      "Loading file 2023-08-26.csv...\n",
      "Loading file 2023-08-27.csv...\n",
      "Loading file 2023-08-28.csv...\n",
      "Loading file 2023-08-29.csv...\n",
      "Loading file 2023-08-30.csv...\n",
      "Loading file 2023-08-31.csv...\n"
     ]
    }
   ],
   "source": [
    "# Extract data from csv files.\n",
    "data_extractor = date_range_data_extractor.DateRangeDataExtractor()\n",
    "data_extractor.extract_data(r'./files/', from_date, to_date)\n",
    "data = data_extractor.data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concatenate data and show info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0b1d73b35a75b7f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52389144 entries, 0 to 52389143\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   Time    object \n",
      " 1   Value   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 799.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(data, ignore_index=True, join='inner')\n",
    "print(df.info(), end='\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:45:53.416580300Z",
     "start_time": "2023-11-10T17:45:52.618622300Z"
    }
   },
   "id": "b4b0b868419679a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scale data\n",
    "Change the Time value to datetime format and filter the data to whole seconds and remove all other values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35345f6b33960fc5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Time     Value\n",
      "0  2023-07-01 00:00:00  50.07763\n",
      "1  2023-07-01 00:00:01  50.07575\n",
      "2  2023-07-01 00:00:02  50.07244\n",
      "3  2023-07-01 00:00:03  50.07040\n",
      "4  2023-07-01 00:00:04  50.07193\n",
      "5  2023-07-01 00:00:05  50.06990\n",
      "6  2023-07-01 00:00:06  50.07419\n",
      "7  2023-07-01 00:00:07  50.07456\n",
      "8  2023-07-01 00:00:08  50.07545\n",
      "9  2023-07-01 00:00:09  50.08005\n",
      "10 2023-07-01 00:00:10  50.08031\n",
      "11 2023-07-01 00:00:11  50.08071\n",
      "12 2023-07-01 00:00:12  50.08263\n",
      "13 2023-07-01 00:00:13  50.08107\n",
      "14 2023-07-01 00:00:14  50.07819\n",
      "15 2023-07-01 00:00:15  50.07428\n",
      "16 2023-07-01 00:00:16  50.06272\n",
      "17 2023-07-01 00:00:17  50.05571\n",
      "18 2023-07-01 00:00:18  50.05046\n",
      "19 2023-07-01 00:00:19  50.04753\n"
     ]
    }
   ],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df = df[df['Time'].dt.microsecond == 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head(20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:36.030353700Z",
     "start_time": "2023-11-10T17:45:53.425553600Z"
    }
   },
   "id": "3128bda800186933"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze integrity of data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ded2bd4c34f093"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check and drop duplicates if any"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce28ba9a762671f2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: \n",
      "8\n",
      "                       Time     Value\n",
      "3601    2023-07-01 01:00:00  49.92183\n",
      "7202    2023-07-01 02:00:00  50.04779\n",
      "10803   2023-07-01 03:00:00  50.07014\n",
      "14404   2023-07-01 04:00:00  49.94890\n",
      "2574519 2023-08-01 01:00:00  50.10892\n",
      "2578120 2023-08-01 02:00:00  50.05931\n",
      "2581721 2023-08-01 03:00:00  50.01276\n",
      "2585322 2023-08-01 04:00:00  49.90219\n"
     ]
    }
   ],
   "source": [
    "duplicated_rows = df[df.duplicated()]\n",
    "num_duplicated = len(duplicated_rows)\n",
    "print(f'Duplicates: \\n{num_duplicated}\\n{duplicated_rows}', end='\\n\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:38.663821600Z",
     "start_time": "2023-11-10T17:47:36.034105800Z"
    }
   },
   "id": "518577c5a5102b06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove duplicates"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fce57f047da4c1bd"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:41.569810200Z",
     "start_time": "2023-11-10T17:47:38.666895200Z"
    }
   },
   "id": "31c4e11ed626341"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Find duplicates in date"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fcf67165c864fa1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in Time column: \n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [Time, Value]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicated_rows_time = df[df.duplicated(subset=\"Time\", keep=False)]\n",
    "num_duplicated_time = len(duplicated_rows_time)\n",
    "print(f'Duplicates in Time column: \\n{num_duplicated_time}\\n{duplicated_rows_time}', end='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:42.728055300Z",
     "start_time": "2023-11-10T17:47:41.573760700Z"
    }
   },
   "id": "e4e93e062ea15efd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove duplicates"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3abf53207780187a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='Time')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:43.942990800Z",
     "start_time": "2023-11-10T17:47:42.719860Z"
    }
   },
   "id": "d0da198db305f804"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Any NaN, Null, 0 or \"\" found in Value."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0b19c4f8f538a62"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is not a number in value column:: \n",
      "0\n",
      "Is a NULL in value column:: \n",
      "0\n",
      "Zero values in value column: \n",
      "0\n",
      "White spaces in value column: \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dfNaN = df[df['Value'].isna()]\n",
    "nrNaN = len(dfNaN)\n",
    "dfNull = df[df['Value'].isnull()]\n",
    "nrNull = len(dfNull)\n",
    "dfZeroValues = df[df['Value'] == 0]\n",
    "zeroValues = len(dfZeroValues)\n",
    "dfWhiteSpaces = df[df['Value'] == \"\"]\n",
    "whiteSpaces = len(dfWhiteSpaces)\n",
    "print(f'Is not a number in value column:: \\n{nrNaN}', end='\\n')\n",
    "print(f'Is a NULL in value column:: \\n{nrNull}', end='\\n')\n",
    "print(f'Zero values in value column: \\n{zeroValues}', end='\\n')\n",
    "print(f'White spaces in value column: \\n{whiteSpaces}', end='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:44.101387800Z",
     "start_time": "2023-11-10T17:47:44.022572700Z"
    }
   },
   "id": "4420e2005b519f84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finding any missing date in the series of dates"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3ec566000febccf"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing dates: 0\n",
      "\n",
      "\n",
      "                  Time     Value\n",
      "0  2023-07-01 00:00:00  50.07763\n",
      "1  2023-07-01 00:00:01  50.07575\n",
      "2  2023-07-01 00:00:02  50.07244\n",
      "3  2023-07-01 00:00:03  50.07040\n",
      "4  2023-07-01 00:00:04  50.07193\n",
      "5  2023-07-01 00:00:05  50.06990\n",
      "6  2023-07-01 00:00:06  50.07419\n",
      "7  2023-07-01 00:00:07  50.07456\n",
      "8  2023-07-01 00:00:08  50.07545\n",
      "9  2023-07-01 00:00:09  50.08005\n",
      "10 2023-07-01 00:00:10  50.08031\n",
      "11 2023-07-01 00:00:11  50.08071\n",
      "12 2023-07-01 00:00:12  50.08263\n",
      "13 2023-07-01 00:00:13  50.08107\n",
      "14 2023-07-01 00:00:14  50.07819\n",
      "15 2023-07-01 00:00:15  50.07428\n",
      "16 2023-07-01 00:00:16  50.06272\n",
      "17 2023-07-01 00:00:17  50.05571\n",
      "18 2023-07-01 00:00:18  50.05046\n",
      "19 2023-07-01 00:00:19  50.04753\n"
     ]
    }
   ],
   "source": [
    "date_range = pd.date_range(start=from_date, end=to_date, freq='S')\n",
    "\n",
    "df.set_index('Time', inplace=True)\n",
    "df = df.reindex(date_range)\n",
    "missingDates = df[df.index.isna()].shape[0] # TODO This code is note doing what it is supose to do. Fix it!!\n",
    "print(f'Number of missing dates: {missingDates}', end='\\n\\n\\n')\n",
    "df.reset_index(inplace=True, names=\"Time\")\n",
    "df['Value'].fillna(-1, inplace=True)\n",
    "print(df.head(20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:46.024867400Z",
     "start_time": "2023-11-10T17:47:44.085808Z"
    }
   },
   "id": "8419bff775d5037c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cell below to filter out time in a range"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf8bfab5e30da34"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# start_time = '2021-12-12 06:00:00'\n",
    "# end_time = '2021-12-12 07:00:00'\n",
    "# filtered_data_time = filtered_df[(filtered_df['Time'] >= start_time) & (filtered_df['Time'] <= end_time)]\n",
    "# print(filtered_data_time.head(20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:47:46.125185Z",
     "start_time": "2023-11-10T17:47:46.026866800Z"
    }
   },
   "id": "fee5ef0d1b051272"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c21e456624cb41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save the data to a new csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c98e1ab88a812cf"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "folder_name = 'processed_files'\n",
    "file_name = output_filename\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "df.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:48:30.897986800Z",
     "start_time": "2023-11-10T17:47:46.057194500Z"
    }
   },
   "id": "92eceeb0d5db51a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save to logfile"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ebeefd6040f2468"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Index    DateFrom      DateTo  NULL  NaN  Exact duplicates  \\\n",
      "0   2021-Q1_seconds.csv  2021-01-01  2021-03-31     0    0                12   \n",
      "1   2021-Q2_seconds.csv  2021-04-01  2021-06-30     0    0                11   \n",
      "2   2021-Q3_seconds.csv  2021-07-01  2021-09-30     0    0                 9   \n",
      "3   2021-Q4_seconds.csv  2021-10-01  2021-12-31     0    0                23   \n",
      "4   2022-Q1_seconds.csv  2022-01-01  2022-03-31     0    0                10   \n",
      "5   2022-Q2_seconds.csv  2022-04-01  2022-06-30     0    0                 5   \n",
      "6   2022-Q3_seconds.csv  2022-07-01  2022-09-30     0    0                 8   \n",
      "7   2022-Q4_seconds.csv  2022-10-01  2022-12-31     0    0                11   \n",
      "8   2023-Q1_seconds.csv  2023-01-01  2023-03-31     0    0                12   \n",
      "9   2023-Q2_seconds.csv  2023-04-01  2023-06-30     0    0                12   \n",
      "10  2023-Q3_seconds.csv  2023-07-01  2023-09-30     0    0                 8   \n",
      "\n",
      "    Time duplicates  Zero Values  White Space  Added missing dates  \n",
      "0                 0            0            0                    0  \n",
      "1                 2            0            0                    0  \n",
      "2                 6            0            0                    0  \n",
      "3              7178            0            0                    0  \n",
      "4                 4            0            0                    0  \n",
      "5                14            0            0                    0  \n",
      "6                 8            0            0                    0  \n",
      "7              7202            0            0                    0  \n",
      "8                 0            0            0                    0  \n",
      "9                 0            0            0                    0  \n",
      "10                0            0            0                    0  \n"
     ]
    }
   ],
   "source": [
    "folder_name = 'log'\n",
    "file_name = 'log.csv'\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "columns = ['Index', 'DateFrom', 'DateTo', 'NULL', 'NaN', 'Exact duplicates', 'Time duplicates', 'Zero Values', 'White Space', 'Added missing dates']\n",
    "\n",
    "new_data = {'Index': output_filename, 'DateFrom': from_date, 'DateTo': to_date, 'NULL': nrNull, 'NaN': nrNaN, 'Exact duplicates': num_duplicated, 'Time duplicates': num_duplicated_time, 'Zero Values': zeroValues, 'White Space': whiteSpaces, 'Added missing dates': missingDates}\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "else:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "index_exists = (df['Index'] == new_data['Index']).any()\n",
    "\n",
    "if index_exists:\n",
    "    df.loc[df['Index'] == new_data['Index']] = [new_data[col] for col in columns]\n",
    "else:\n",
    "    new_row = pd.DataFrame([new_data], columns=columns)\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:48:30.965126500Z",
     "start_time": "2023-11-10T17:48:30.909732200Z"
    }
   },
   "id": "24b9d6dc0210ff17"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                  Index    DateFrom      DateTo  NULL  NaN  Exact duplicates  \\\n0   2021-Q1_seconds.csv  2021-01-01  2021-03-31     0    0                12   \n1   2021-Q2_seconds.csv  2021-04-01  2021-06-30     0    0                11   \n2   2021-Q3_seconds.csv  2021-07-01  2021-09-30     0    0                 9   \n3   2021-Q4_seconds.csv  2021-10-01  2021-12-31     0    0                23   \n4   2022-Q1_seconds.csv  2022-01-01  2022-03-31     0    0                10   \n5   2022-Q2_seconds.csv  2022-04-01  2022-06-30     0    0                 5   \n6   2022-Q3_seconds.csv  2022-07-01  2022-09-30     0    0                 8   \n7   2022-Q4_seconds.csv  2022-10-01  2022-12-31     0    0                11   \n8   2023-Q1_seconds.csv  2023-01-01  2023-03-31     0    0                12   \n9   2023-Q2_seconds.csv  2023-04-01  2023-06-30     0    0                12   \n10  2023-Q3_seconds.csv  2023-07-01  2023-09-30     0    0                 8   \n\n    Time duplicates  Zero Values  White Space  Added missing dates  \n0                 0            0            0                    0  \n1                 2            0            0                    0  \n2                 6            0            0                    0  \n3              7178            0            0                    0  \n4                 4            0            0                    0  \n5                14            0            0                    0  \n6                 8            0            0                    0  \n7              7202            0            0                    0  \n8                 0            0            0                    0  \n9                 0            0            0                    0  \n10                0            0            0                    0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>DateFrom</th>\n      <th>DateTo</th>\n      <th>NULL</th>\n      <th>NaN</th>\n      <th>Exact duplicates</th>\n      <th>Time duplicates</th>\n      <th>Zero Values</th>\n      <th>White Space</th>\n      <th>Added missing dates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-Q1_seconds.csv</td>\n      <td>2021-01-01</td>\n      <td>2021-03-31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-Q2_seconds.csv</td>\n      <td>2021-04-01</td>\n      <td>2021-06-30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-Q3_seconds.csv</td>\n      <td>2021-07-01</td>\n      <td>2021-09-30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-Q4_seconds.csv</td>\n      <td>2021-10-01</td>\n      <td>2021-12-31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23</td>\n      <td>7178</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-Q1_seconds.csv</td>\n      <td>2022-01-01</td>\n      <td>2022-03-31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2022-Q2_seconds.csv</td>\n      <td>2022-04-01</td>\n      <td>2022-06-30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2022-Q3_seconds.csv</td>\n      <td>2022-07-01</td>\n      <td>2022-09-30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2022-Q4_seconds.csv</td>\n      <td>2022-10-01</td>\n      <td>2022-12-31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>7202</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2023-Q1_seconds.csv</td>\n      <td>2023-01-01</td>\n      <td>2023-03-31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2023-Q2_seconds.csv</td>\n      <td>2023-04-01</td>\n      <td>2023-06-30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2023-Q3_seconds.csv</td>\n      <td>2023-07-01</td>\n      <td>2023-09-30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T17:57:15.848269800Z",
     "start_time": "2023-11-10T17:57:15.715625900Z"
    }
   },
   "id": "60694f23c73820f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f84ae64f47f68bfe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
